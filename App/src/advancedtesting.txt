#!/usr/bin/env python3

"""
DDoS Protection Layer Testing Suite
Specifically designed for testing the Analyzer API capacity and performance
"""

import requests
import time
import csv
import threading
import random
from datetime import datetime
import concurrent.futures
import psutil
import os
import sys

class AnalyzerAPITester:
    def __init__(self, analyzer_url):
        self.analyzer_url = analyzer_url
        self.results = []
        self.lock = threading.Lock()
        self.start_time = None
        self.test_running = False
        
        # Test IP pools for different scenarios
        self.normal_ips = [
            "192.168.1.100", "10.0.0.15", "172.16.0.45", "203.0.113.12",
            "198.51.100.8", "192.0.2.150", "10.10.10.25", "172.31.255.1",
            "192.168.100.50", "203.0.113.200", "198.51.100.99", "192.0.2.75",
            "10.255.255.100", "172.20.0.33", "192.168.50.200"
        ]
        
        self.suspicious_ips = [
            "1.1.1.1", "8.8.8.8", "127.0.0.1", "0.0.0.0",
            "255.255.255.255", "192.168.1.1", "10.0.0.1"
        ]
        
        self.user_agents = [
            "TestBot", "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
            "curl/7.68.0", "Python-requests/2.25.1", "Postman",
            "DDoSBot", "AttackTool", "Scanner", ""
        ]

    def generate_random_ip(self):
        """Generate a random IP address"""
        return f"{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}.{random.randint(1,255)}"

    def make_api_request(self, ip, user_agent, request_id):
        """Make a single request to the analyzer API"""
        headers = {
            'X-Forwarded-For': ip,
            'User-Agent': user_agent
        }
        
        start_time = time.time()
        
        try:
            response = requests.get(self.analyzer_url, headers=headers)
            end_time = time.time()
            
            result = response.json() if response.content else {}
            
            request_data = {
                'request_id': request_id,
                'timestamp': datetime.now().isoformat(),
                'source_ip': ip,
                'user_agent': user_agent,
                'status_code': response.status_code,
                'response_time': end_time - start_time,
                'is_suspicious': result.get('is_suspicious', False),
                'success': True,
                'error': None
            }
            
        except requests.exceptions.RequestException as e:
            end_time = time.time()
            request_data = {
                'request_id': request_id,
                'timestamp': datetime.now().isoformat(),
                'source_ip': ip,
                'user_agent': user_agent,
                'status_code': 0,
                'response_time': end_time - start_time,
                'is_suspicious': False,
                'success': False,
                'error': str(e)
            }
        
        with self.lock:
            self.results.append(request_data)
        
        return request_data

    def functional_test(self):
        """Test basic functionality with known IPs"""
        print(" Running Functional Test...")
        print("   Testing basic API functionality with known IP addresses")
        
        test_cases = [
            ('Normal IP 1', self.normal_ips[0], 'TestBot'),
            ('Normal IP 2', self.normal_ips[1], 'Mozilla/5.0'),
            ('Suspicious IP', '1.1.1.1', 'DDoSBot'),
            ('Empty User Agent', self.normal_ips[2], ''),
            ('Random IP', self.generate_random_ip(), 'curl/7.68.0')
        ]
        
        for test_name, ip, user_agent in test_cases:
            print(f"   Testing: {test_name} ({ip})")
            self.make_api_request(ip, user_agent, f"FUNC_{test_name}")
            time.sleep(0.5)  # Small delay between functional tests
        
        print(" Functional Test Complete")

    def gradual_load_test(self, max_concurrent=100, step=10):
        """Gradually increase concurrent requests"""
        print(f"Running Gradual Load Test...")
        print(f"   Testing from 1 to {max_concurrent} concurrent requests (step: {step})")
        
        for concurrent_users in range(step, max_concurrent + 1, step):
            print(f"   Testing with {concurrent_users} concurrent requests...")
            
            # Record system stats before each batch
            cpu_before = psutil.cpu_percent()
            memory_before = psutil.virtual_memory().percent
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_users) as executor:
                futures = []
                
                for i in range(concurrent_users):
                    ip = random.choice(self.normal_ips + [self.generate_random_ip()])
                    user_agent = random.choice(self.user_agents)
                    request_id = f"GRAD_{concurrent_users}_{i+1}"
                    
                    future = executor.submit(self.make_api_request, ip, user_agent, request_id)
                    futures.append(future)
                
                # Wait for all requests to complete
                concurrent.futures.as_completed(futures)
            
            # Brief pause between batches
            time.sleep(2)
        
        print(" Gradual Load Test Complete")

    def load_test(self, concurrent_users=50, duration=60):
        """Sustained load test"""
        print(f" Running Load Test...")
        print(f"   {concurrent_users} concurrent users for {duration} seconds")
        
        end_time = time.time() + duration
        request_count = 0
        
        while time.time() < end_time:
            with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_users) as executor:
                futures = []
                
                for i in range(concurrent_users):
                    ip = random.choice(self.normal_ips + [self.generate_random_ip()])
                    user_agent = random.choice(self.user_agents)
                    request_id = f"LOAD_{request_count + i + 1}"
                    
                    future = executor.submit(self.make_api_request, ip, user_agent, request_id)
                    futures.append(future)
                
                # Wait for batch to complete
                for future in concurrent.futures.as_completed(futures):
                    pass
                
                request_count += concurrent_users
                
                # Show progress
                remaining = int(end_time - time.time())
                print(f"   Progress: {request_count} requests sent, {remaining}s remaining...")
        
        print(" Load Test Complete")

    def burst_test(self, burst_size=200):
        """High-intensity burst test"""
        print(f" Running Burst Test...")
        print(f" Sending {burst_size} requests simultaneously")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=burst_size) as executor:
            futures = []
            
            for i in range(burst_size):
                # Mix of normal and suspicious IPs
                if i % 4 == 0:  # 25% suspicious
                    ip = random.choice(self.suspicious_ips)
                    user_agent = random.choice(['DDoSBot', 'AttackTool', 'Scanner'])
                else:
                    ip = random.choice(self.normal_ips + [self.generate_random_ip()])
                    user_agent = random.choice(self.user_agents)
                
                request_id = f"BURST_{i+1}"
                future = executor.submit(self.make_api_request, ip, user_agent, request_id)
                futures.append(future)
            
            # Wait for all requests to complete
            print("   Waiting for all requests to complete...")
            for future in concurrent.futures.as_completed(futures):
                pass
        
        print(" Burst Test Complete")

    def monitor_system_resources(self):
        """Monitor system resources during tests"""
        resource_data = []
        
        while self.test_running:
            timestamp = datetime.now().isoformat()
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            
            resource_data.append({
                'timestamp': timestamp,
                'cpu_percent': cpu_percent,
                'memory_percent': memory.percent,
                'memory_used_mb': memory.used / 1024 / 1024,
                'active_requests': len([r for r in self.results if r['timestamp'] > (datetime.now().timestamp() - 5)])
            })
        
        return resource_data

    def save_results(self, filename):
        """Save test results to CSV"""
        if not self.results:
            print(" No results to save")
            return
        
        fieldnames = [
            'request_id', 'timestamp', 'source_ip', 'user_agent',
            'status_code', 'response_time', 'is_suspicious', 'success', 'error'
        ]
        
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(self.results)
        
        print(f" Results saved to: {filename}")

    def generate_report(self):
        """Generate comprehensive test report"""
        if not self.results:
            print(" No results to analyze")
            return
        
        total_requests = len(self.results)
        successful_requests = len([r for r in self.results if r['success']])
        failed_requests = total_requests - successful_requests
        suspicious_flagged = len([r for r in self.results if r['is_suspicious']])
        
        # Response time analysis
        successful_times = [r['response_time'] for r in self.results if r['success']]
        
        if successful_times:
            avg_response_time = sum(successful_times) / len(successful_times)
            max_response_time = max(successful_times)
            min_response_time = min(successful_times)
            
            # Calculate percentiles
            successful_times.sort()
            p50 = successful_times[len(successful_times) // 2]
            p95 = successful_times[int(len(successful_times) * 0.95)]
            p99 = successful_times[int(len(successful_times) * 0.99)]
        
        # Status code analysis
        status_codes = {}
        for result in self.results:
            code = result['status_code']
            status_codes[code] = status_codes.get(code, 0) + 1
        
        # Test type breakdown
        test_types = {}
        for result in self.results:
            test_type = result['request_id'].split('_')[0] if '_' in result['request_id'] else 'UNKNOWN'
            test_types[test_type] = test_types.get(test_type, 0) + 1
        
        print("\n" + "="*60)
        print(" ANALYZER API TEST REPORT")
        print("="*60)
        print(f" Target API: {self.analyzer_url}")
        print(f" Test Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f" Total Test Duration: {time.time() - self.start_time:.1f} seconds")
        
        print(f"\n📈 OVERALL STATISTICS")
        print(f"Total Requests Sent: {total_requests:,}")
        print(f"Successful Requests: {successful_requests:,} ({successful_requests/total_requests*100:.1f}%)")
        print(f"Failed Requests: {failed_requests:,} ({failed_requests/total_requests*100:.1f}%)")
        print(f"Flagged as Suspicious: {suspicious_flagged:,} ({suspicious_flagged/total_requests*100:.1f}%)")
        
        if successful_times:
            print(f" RESPONSE TIME ANALYSIS")
            print(f"Average Response Time: {avg_response_time:.3f}s")
            print(f"Median Response Time (P50): {p50:.3f}s")
            print(f"95th Percentile (P95): {p95:.3f}s")
            print(f"99th Percentile (P99): {p99:.3f}s")
            print(f"Fastest Response: {min_response_time:.3f}s")
            print(f"Slowest Response: {max_response_time:.3f}s")
        
        print(f"STATUS CODE BREAKDOWN")
        for code, count in sorted(status_codes.items()):
            percentage = (count / total_requests) * 100
            status_name = {200: "OK", 0: "Failed/Timeout", 500: "Server Error", 403: "Forbidden"}.get(code, "Unknown")
            print(f"  {code} ({status_name}): {count:,} ({percentage:.1f}%)")
        
        print(f"\n TEST TYPE BREAKDOWN")
        for test_type, count in sorted(test_types.items()):
            percentage = (count / total_requests) * 100
            print(f"  {test_type}: {count:,} requests ({percentage:.1f}%)")
        
        # Performance thresholds
        print(f"\n  PERFORMANCE ANALYSIS")
        if successful_times:
            slow_requests = len([t for t in successful_times if t > 1.0])
            very_slow_requests = len([t for t in successful_times if t > 5.0])
            
            print(f"Requests > 1 second: {slow_requests:,} ({slow_requests/len(successful_times)*100:.1f}%)")
            print(f"Requests > 5 seconds: {very_slow_requests:,} ({very_slow_requests/len(successful_times)*100:.1f}%)")
            
            if avg_response_time > 2.0:
                print(" WARNING: Average response time is high (>2s)")
            elif avg_response_time > 1.0:
                print("  CAUTION: Average response time is elevated (>1s)")
            else:
                print(" GOOD: Average response time is acceptable (<1s)")
        
        # Capacity estimation
        if successful_times and avg_response_time > 0:
            estimated_rps = 1 / avg_response_time
            print(f"\n CAPACITY ESTIMATION")
            print(f"Estimated Max Requests/Second: ~{estimated_rps:.0f} RPS")
            print(f"Estimated Daily Capacity: ~{estimated_rps * 86400:,.0f} requests/day")

    def run_all_tests(self):
        """FOr running all the test suite"""
        print("Starting Analyzer API Testing Suite")
        print(f" Target: {self.analyzer_url}")
        print("="*60)
        
        self.start_time = time.time()
        self.test_running = True
        
        try:
            # Run all test types
            self.functional_test()
            print()
            
            self.gradual_load_test(max_concurrent=50, step=10)
            print()
            
            self.load_test(concurrent_users=30, duration=30)
            print()
            
            self.burst_test(burst_size=100)
            print()
            
        except KeyboardInterrupt:
            print("\n Testing interrupted by user")
        finally:
            self.test_running = False
        
        # Generate report and save results
        self.generate_report()
        
        # Save to CSV with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"analyzer_api_test_results_{timestamp}.csv"
        self.save_results(filename)

def main():
    # Configuration
    ANALYZER_URL = "http://192.128.23.144:8081/api/analyze-now"
    
    print("Analyzer API Testing Suite")
    print("Testing API capacity and performance")
    print("-" * 40)
    
    # Check if API is accessible
    print(f"Checking API accessibility...")
    try:
        response = requests.get(ANALYZER_URL, timeout=5)
        print(f"API is accessible (Status: {response.status_code})")
    except Exception as e:
        print(f" API is not accessible: {e}")
        print("Please check the URL and make sure the service is running")
        return
    
    # Create tester instance
    tester = AnalyzerAPITester(ANALYZER_URL)
    
    # Run the complete test suite
    tester.run_all_tests()
    
    print("Testing Complete!")
    print("Check the generated CSV file for detailed results.")

if __name__ == "__main__":
    main()